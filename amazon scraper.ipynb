{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c23808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of pages to scrape: 1\n",
      "Scraping page 1...\n",
      "Data has been saved to 'amazon_laptop_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "# Functions to extract product details\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", attrs={\"id\": \"productTitle\"}).text.strip()\n",
    "        return title\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={\"class\": \"a-price-whole\"}).text.strip()\n",
    "        return price\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"span\", attrs={\"class\": \"a-icon-alt\"}).text.strip()\n",
    "        return rating\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={\"id\": \"acrCustomerReviewText\"}).text.strip()\n",
    "        return review_count\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        availability = soup.find(\"div\", attrs={\"id\": \"availability\"}).find(\"span\").text.strip()\n",
    "        return availability\n",
    "    except AttributeError:\n",
    "        return \"Not Available\"\n",
    "\n",
    "def get_product_url(link):\n",
    "    return \"https://www.amazon.in\" + link\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Add your user agent\n",
    "    HEADERS = ({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    })\n",
    "\n",
    "    # Base URL (Without page number)\n",
    "    BASE_URL = \"https://www.amazon.in/s?k=laptops&crid=33BYNY0AIDK93&sprefix=laptops%2Caps%2C587&ref=nb_sb_noss_2\"\n",
    "\n",
    "    # Get the number of pages to scrape\n",
    "    num_pages = int(input(\"Enter the number of pages to scrape: \"))\n",
    "\n",
    "    # Data dictionary to store product details\n",
    "    data = {\"title\": [], \"price\": [], \"rating\": [], \"reviews\": [], \"availability\": [], \"product_url\": []}\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        URL = f\"{BASE_URL}&page={page}\"\n",
    "\n",
    "        # HTTP Request with retry mechanism\n",
    "        for attempt in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                webpage = requests.get(URL, headers=HEADERS, timeout=10)\n",
    "                webpage.raise_for_status()\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1} failed for {URL}: {e}\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page} after multiple attempts.\")\n",
    "            continue\n",
    "\n",
    "        # Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "        # Fetch links as List of Tag Objects\n",
    "        links = soup.find_all(\"a\", attrs={'class': 'a-link-normal s-no-outline'})\n",
    "\n",
    "        # Store the links\n",
    "        links_list = []\n",
    "\n",
    "        # Extract valid links\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith(\"/\"):\n",
    "                links_list.append(href)\n",
    "\n",
    "        # Loop for extracting product details from each link\n",
    "        for link in links_list:\n",
    "            product_url = get_product_url(link)\n",
    "\n",
    "            # HTTP Request for product page with retry mechanism\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    product_page = requests.get(product_url, headers=HEADERS, timeout=10)\n",
    "                    product_page.raise_for_status()\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Attempt {attempt + 1} failed for {product_url}: {e}\")\n",
    "                    time.sleep(2)  # Wait before retrying\n",
    "            else:\n",
    "                print(f\"Failed to fetch product page {product_url} after multiple attempts.\")\n",
    "                continue\n",
    "\n",
    "            product_soup = BeautifulSoup(product_page.content, \"html.parser\")\n",
    "\n",
    "            # Extract and append product details\n",
    "            data['title'].append(get_title(product_soup))\n",
    "            data['price'].append(get_price(product_soup))\n",
    "            data['rating'].append(get_rating(product_soup))\n",
    "            data['reviews'].append(get_review_count(product_soup))\n",
    "            data['availability'].append(get_availability(product_soup))\n",
    "            data['product_url'].append(product_url)\n",
    "\n",
    "            # Pause to reduce request frequency (randomized delay)\n",
    "            time.sleep(uniform(1, 3))\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    amazon_df = pd.DataFrame.from_dict(data)\n",
    "    amazon_df['title'] = amazon_df['title'].replace('', np.nan)\n",
    "    amazon_df.dropna(subset=['title'], inplace=True)\n",
    "    amazon_df.to_csv(\"amazon_laptop_data.csv\", header=True, index=False)\n",
    "\n",
    "    print(\"Data has been saved to 'amazon_laptop_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47651769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
